{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\n\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\nimport os\nprint(os.listdir(\"../input\"))\n","metadata":{"execution":{"iopub.status.busy":"2023-04-10T15:59:37.066288Z","iopub.execute_input":"2023-04-10T15:59:37.066867Z","iopub.status.idle":"2023-04-10T15:59:37.087355Z","shell.execute_reply.started":"2023-04-10T15:59:37.066793Z","shell.execute_reply":"2023-04-10T15:59:37.086340Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"['bank.csv']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true}},{"cell_type":"code","source":"#!pip install pyspark","metadata":{"execution":{"iopub.status.busy":"2023-04-10T16:00:11.724111Z","iopub.execute_input":"2023-04-10T16:00:11.724632Z","iopub.status.idle":"2023-04-10T16:00:11.728310Z","shell.execute_reply.started":"2023-04-10T16:00:11.724382Z","shell.execute_reply":"2023-04-10T16:00:11.727592Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('ml-bank').getOrCreate()\ndf = spark.read.csv('../input/bank.csv', header = True, inferSchema = True)\ndf.printSchema()","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2023-04-10T15:59:44.266944Z","iopub.execute_input":"2023-04-10T15:59:44.267273Z","iopub.status.idle":"2023-04-10T15:59:44.283627Z","shell.execute_reply.started":"2023-04-10T15:59:44.267215Z","shell.execute_reply":"2023-04-10T15:59:44.281759Z"},"trusted":true},"execution_count":8,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-64f370668901>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ml-bank'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/bank.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"],"ename":"ModuleNotFoundError","evalue":"No module named 'pyspark'","output_type":"error"}]},{"cell_type":"code","source":"import pandas as pd\npd.DataFrame(df.take(5), columns=df.columns).transpose()","metadata":{"execution":{"iopub.status.busy":"2023-04-10T15:59:51.128911Z","iopub.execute_input":"2023-04-10T15:59:51.129241Z","iopub.status.idle":"2023-04-10T15:59:51.143338Z","shell.execute_reply.started":"2023-04-10T15:59:51.129194Z","shell.execute_reply":"2023-04-10T15:59:51.142035Z"},"trusted":true},"execution_count":9,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-57d3c0e5e86b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"],"ename":"NameError","evalue":"name 'df' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"**Analyzing Data**","metadata":{}},{"cell_type":"code","source":"df.groupby('deposit').count().toPandas()","metadata":{"execution":{"iopub.status.busy":"2023-04-10T15:26:52.974302Z","iopub.status.idle":"2023-04-10T15:26:52.974904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numeric_features = [t[0] for t in df.dtypes if t[1] == 'int']\ndf.select(numeric_features).describe().toPandas().transpose()","metadata":{"execution":{"iopub.status.busy":"2023-04-10T15:26:52.976456Z","iopub.status.idle":"2023-04-10T15:26:52.977104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numeric_data = df.select(numeric_features).toPandas()\naxs = pd.scatter_matrix(numeric_data, figsize=(8, 8));\nn = len(numeric_data.columns)\nfor i in range(n):\n    v = axs[i, 0]\n    v.yaxis.label.set_rotation(0)\n    v.yaxis.label.set_ha('right')\n    v.set_yticks(())\n    h = axs[n-1, i]\n    h.xaxis.label.set_rotation(90)\n    h.set_xticks(())\n","metadata":{"execution":{"iopub.status.busy":"2023-04-10T15:26:52.978460Z","iopub.status.idle":"2023-04-10T15:26:52.979033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.select('age', 'job', 'marital', 'education', 'default', 'balance', 'housing', 'loan', 'contact', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'deposit')\ncols = df.columns\ndf.printSchema()","metadata":{"execution":{"iopub.status.busy":"2023-04-10T15:26:52.980409Z","iopub.status.idle":"2023-04-10T15:26:52.981240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n\ncategoricalColumns = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'poutcome']\n\nstages = []\n\nfor categoricalCol in categoricalColumns:\n    \n    stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')\n    \n    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    \n    stages += [stringIndexer, encoder]\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-10T15:26:52.982479Z","iopub.status.idle":"2023-04-10T15:26:52.983225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We use the StringIndexer again to encode our labels to label indices. Next, we use the VectorAssembler to combine all the feature columns into a single vector column.","metadata":{}},{"cell_type":"code","source":"label_stringIdx = StringIndexer(inputCol = 'deposit', outputCol = 'label')\n\nstages += [label_stringIdx]\n\nnumericCols = ['age', 'balance', 'duration', 'campaign', 'pdays', 'previous']\n\nassemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\n\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n\nstages += [assembler]","metadata":{"execution":{"iopub.status.busy":"2023-04-10T15:26:52.984580Z","iopub.status.idle":"2023-04-10T15:26:52.985135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Pipeline**\n\nWe use Pipeline to chain multiple Transformers and Estimators together to specify our machine learning workflow. A Pipelineâ€™s stages are specified as an ordered array.","metadata":{}},{"cell_type":"code","source":"from pyspark.ml import Pipeline\npipeline = Pipeline(stages = stages)\npipelineModel = pipeline.fit(df)\ndf = pipelineModel.transform(df)\nselectedCols = ['label', 'features'] + cols\ndf = df.select(selectedCols)\ndf.printSchema()","metadata":{"execution":{"iopub.status.busy":"2023-04-10T15:26:52.986777Z","iopub.status.idle":"2023-04-10T15:26:52.987790Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(df.take(5), columns=df.columns).transpose()","metadata":{"execution":{"iopub.status.busy":"2023-04-10T15:26:52.989236Z","iopub.status.idle":"2023-04-10T15:26:52.989783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train, test = df.randomSplit([0.7, 0.3], seed = 2018)\nprint(\"Training Dataset Count: \" + str(train.count()))\nprint(\"Test Dataset Count: \" + str(test.count()))","metadata":{"execution":{"iopub.status.busy":"2023-04-10T15:26:52.991233Z","iopub.status.idle":"2023-04-10T15:26:52.991840Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Creating Logistic Regression Model**","metadata":{}},{"cell_type":"code","source":"from pyspark.ml.classification import LogisticRegression\nlr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10)\nlrModel = lr.fit(train)","metadata":{"execution":{"iopub.status.busy":"2023-04-10T15:26:52.993375Z","iopub.status.idle":"2023-04-10T15:26:52.993918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nbeta = np.sort(lrModel.coefficients)\nplt.plot(beta)\nplt.ylabel('Beta Coefficients')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-10T15:26:52.995725Z","iopub.status.idle":"2023-04-10T15:26:52.996776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainingSummary = lrModel.summary\nroc = trainingSummary.roc.toPandas()\nplt.plot(roc['FPR'],roc['TPR'])\nplt.ylabel('False Positive Rate')\nplt.xlabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.show()\nprint('Training set areaUnderROC: ' + str(trainingSummary.areaUnderROC))","metadata":{"execution":{"iopub.status.busy":"2023-04-10T15:26:52.998053Z","iopub.status.idle":"2023-04-10T15:26:52.998618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Precision and Recall**","metadata":{}},{"cell_type":"code","source":"pr = trainingSummary.pr.toPandas()\nplt.plot(pr['recall'],pr['precision'])\nplt.ylabel('Precision')\nplt.xlabel('Recall')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-10T15:26:53.101253Z","iopub.execute_input":"2023-04-10T15:26:53.101596Z","iopub.status.idle":"2023-04-10T15:26:53.115552Z","shell.execute_reply.started":"2023-04-10T15:26:53.101532Z","shell.execute_reply":"2023-04-10T15:26:53.114294Z"},"trusted":true},"execution_count":4,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-4452c4c2fb05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainingSummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'recall'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'precision'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Precision'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Recall'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'trainingSummary' is not defined"],"ename":"NameError","evalue":"name 'trainingSummary' is not defined","output_type":"error"}]},{"cell_type":"code","source":"predictions = lrModel.transform(test)\npredictions.select('age', 'job', 'label', 'rawPrediction', 'prediction', 'probability').show(10)","metadata":{"execution":{"iopub.status.busy":"2023-04-10T15:26:53.116822Z","iopub.status.idle":"2023-04-10T15:26:53.117364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Evaluate Logistic Regression**","metadata":{}},{"cell_type":"code","source":"from pyspark.ml.evaluation import BinaryClassificationEvaluator\nevaluator = BinaryClassificationEvaluator()\nprint('Test Area Under ROC', evaluator.evaluate(predictions))","metadata":{"execution":{"iopub.status.busy":"2023-04-10T15:26:53.118673Z","iopub.status.idle":"2023-04-10T15:26:53.119248Z"},"trusted":true},"execution_count":null,"outputs":[]}]}